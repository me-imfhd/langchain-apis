{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very imp and useful util\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful util\n",
    "# import sys\n",
    "# import pkg_resources\n",
    "# print(\"Python interpreter:\", sys.executable)\n",
    "# print(\"Packages:\")\n",
    "# for dist in pkg_resources.working_set:\n",
    "#     print(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.notion.so/Week-6-1-React-Deeper-Dive-581a481892fe4d3a8e0cb5360ce8d73e\",),\n",
    "    bs_kwargs=dict(\n",
    "        # this is helps to parse the html given by the url to text\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            # class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.', metadata={'source': 'https://www.notion.so/Week-6-1-React-Deeper-Dive-581a481892fe4d3a8e0cb5360ce8d73e', 'title': 'Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.', 'description': \"A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\", 'language': 'No language found.'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with chat history memory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"You are a chatbot having a conversation with a human your are a {ability}.\n",
    "\n",
    "Given the following extracted parts of a long document and a question,\n",
    "create a final answer using your intelligence.\n",
    "\n",
    "(This context can be completely irrelevant to the question if so, skip it,\n",
    "focus on giving the best answer to the human question)\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "with_history_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    output_key=\"AI\",\n",
    "    max_token_limit=100,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# print(memory.load_memory_variables(\"chat_history\"))\n",
    "custom_chain = (\n",
    "    {\n",
    "        \"chat_history\": itemgetter(\"chat_history\") ,\n",
    "        \"ability\": itemgetter(\"ability\"),\n",
    "        \"human_input\": itemgetter(\"human_input\"),\n",
    "        \"context\": itemgetter(\"input_documents\")\n",
    "    }\n",
    "    # | with_history_prompt\n",
    "    | LLMChain(\n",
    "        llm=llm, \n",
    "        prompt=with_history_prompt,\n",
    "        # verbose=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can you emphasis more on COT?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "docs = format_docs(retriever.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_markdown(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data = custom_chain.invoke(\n",
    "    {\n",
    "        \"input_documents\":docs,\n",
    "        \"chat_history\":memory.load_memory_variables(\"chat_history\")['chat_history'],\n",
    "        \"human_input\": question, \n",
    "        \"ability\": \"Good talker\"\n",
    "    }\n",
    ")\n",
    "memory.save_context({\"human_input\":question},{\"AI\":response_data['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_markdown(memory.load_memory_variables(\"chat_history\")['chat_history']) # this is whats stored as history\n",
    "to_markdown(response_data['text']) # ai response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
