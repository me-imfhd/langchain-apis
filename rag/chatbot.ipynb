{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very imp and useful util\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful util\n",
    "# import sys\n",
    "# import pkg_resources\n",
    "# print(\"Python interpreter:\", sys.executable)\n",
    "# print(\"Packages:\")\n",
    "# for dist in pkg_resources.working_set:\n",
    "#     print(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        # this is helps to parse the html given by the url to text\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with chat history memory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"You are a chatbot having a conversation with a human your are a {ability}.\n",
    "\n",
    "Given the following extracted parts of a long document and a question,\n",
    "create a final answer using your intelligence.\n",
    "\n",
    "(This context can be completely irrelevant to the question if so, skip it,\n",
    "focus on giving the best answer to the human question)\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "with_history_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    output_key=\"AI\",\n",
    "    max_token_limit=100,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# print(memory.load_memory_variables(\"chat_history\"))\n",
    "custom_chain = (\n",
    "    {\n",
    "        \"chat_history\": itemgetter(\"chat_history\") ,\n",
    "        \"ability\": itemgetter(\"ability\"),\n",
    "        \"human_input\": itemgetter(\"human_input\"),\n",
    "        \"context\": itemgetter(\"input_documents\")\n",
    "    }\n",
    "    # | with_history_prompt\n",
    "    | LLMChain(\n",
    "        llm=llm, \n",
    "        prompt=with_history_prompt,\n",
    "        # verbose=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can you emphasis more on COT?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "docs = format_docs(retriever.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_markdown(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data = custom_chain.invoke(\n",
    "    {\n",
    "        \"input_documents\":docs,\n",
    "        \"chat_history\":memory.load_memory_variables(\"chat_history\")['chat_history'],\n",
    "        \"human_input\": question, \n",
    "        \"ability\": \"Good talker\"\n",
    "    }\n",
    ")\n",
    "memory.save_context({\"human_input\":question},{\"AI\":response_data['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> System: The human asks the AI about task decomposition. The AI explains that task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It mentions two approaches to task decomposition, the Chain of Thought technique and the Tree of Thoughts technique. The AI also explains that task decomposition can be done using language models with different methods such as simple prompting, task-specific instructions, or with human inputs. The human asks for more information about the Chain of Thought technique. The AI explains that the Chain of Thought technique is a prompting technique that enhances model performance on complex tasks by instructing the model to \"think step by step\" and decompose hard tasks into smaller and simpler steps. It allows the model to utilize more test-time computation and gain insights into its thinking process. The AI also mentions that the Chain of Thought technique is particularly useful for complicated tasks that involve many steps, as it provides a structured approach to task decomposition and helps an agent plan ahead by providing a clear understanding of the necessary steps to accomplish the task. The AI asks if the human has any more questions about task decomposition or the Chain of Thought technique."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(memory.load_memory_variables(\"chat_history\")['chat_history']) # this is whats stored as history\n",
    "to_markdown(response_data['text']) # ai response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
