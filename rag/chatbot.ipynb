{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cohere tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very imp and useful util\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful util\n",
    "import sys\n",
    "import pkg_resources\n",
    "print(\"Python interpreter:\", sys.executable)\n",
    "print(\"Packages:\")\n",
    "for dist in pkg_resources.working_set:\n",
    "    print(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        # this is helps to parse the html given by the url to text\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "# chunk_siz refers to words, chunk_overlap refers to last 100 words previous chunk and\n",
    "# first 100 words of next chunk\n",
    "# startindex adds indexes to the metadata i guess is helpfull when storing in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "for split in splits:\n",
    "   print(split.metadata)\n",
    "\n",
    "# you can see all indexes that need to be made embeddings to store in vector db maybe, but not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do all your prompt prefrences here\n",
    "# example:\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "# use custom_rag_prompt at place of prompt in rag_chain\n",
    "custom_rag_prompt = PromptTemplate.from_template(rag_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with chat history memory\n",
    "template = \"\"\"You are usually good at {ability}. \n",
    "\n",
    "Here is some context for the question tha can be asked:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant pieces of previous conversation:\n",
    "{history}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "with_history_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose your llm and chatmodel\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "from operator import itemgetter\n",
    "\n",
    "# rag_chain = (\n",
    "#     {\"context\": itemgetter(\"question\") | retriever | format_docs, \"question\": itemgetter(\"question\")}\n",
    "#     | with_history_prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "# def process_chat_history(chat_history):\n",
    "#     history = []\n",
    "    \n",
    "#     for message in chat_history:\n",
    "#         if isinstance(message, AIMessage):\n",
    "#             history.append(f\"AI: {message.content}\")\n",
    "#         elif isinstance(message, HumanMessage):\n",
    "#             history.append(f\"Human: {message.content}\")\n",
    "    \n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-0.0.5-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain_google_genai) (0.1.7)\n",
      "Collecting google-generativeai<0.4.0,>=0.3.1\n",
      "  Downloading google_generativeai-0.3.2-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 KB\u001b[0m \u001b[31m836.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (4.9.0)\n",
      "Requirement already satisfied: tqdm in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (4.66.1)\n",
      "Requirement already satisfied: protobuf in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (4.25.1)\n",
      "Requirement already satisfied: google-auth in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (2.26.1)\n",
      "Collecting google-api-core\n",
      "  Downloading google_api_core-2.15.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.0/122.0 KB\u001b[0m \u001b[31m888.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-ai-generativelanguage==0.4.0\n",
      "  Downloading google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (2.31.0)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (0.0.77)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (23.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (6.0.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (2.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (4.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_google_genai) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_google_genai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_google_genai) (3.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain_google_genai) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain_google_genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain_google_genai) (2.14.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_google_genai) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_google_genai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_google_genai) (1.26.18)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (1.62.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (0.3.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (1.60.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.60.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (0.5.1)\n",
      "Installing collected packages: proto-plus, grpcio-status, google-api-core, google-ai-generativelanguage, google-generativeai, langchain_google_genai\n",
      "Successfully installed google-ai-generativelanguage-0.4.0 google-api-core-2.15.0 google-generativeai-0.3.2 grpcio-status-1.60.0 langchain_google_genai-0.0.5 proto-plus-1.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meimfhd/fhd/langchain-apis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for GoogleGenerativeAIEmbeddings\nmodel\n  field required (type=value_error.missing)\n__root__\n  Did not find google_api_key, please add an environment variable `GOOGLE_API_KEY` which contains it, or pass `google_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1536\u001b[39m \u001b[38;5;66;03m# Dimensions of the OpenAIEmbeddings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(embedding_size)\n\u001b[0;32m---> 10\u001b[0m embedding_fn \u001b[38;5;241m=\u001b[39m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membed_query\n\u001b[1;32m     11\u001b[0m convo_vectorstore \u001b[38;5;241m=\u001b[39m FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n\u001b[1;32m     13\u001b[0m convo_retriever \u001b[38;5;241m=\u001b[39m convo_vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m6\u001b[39m})\n",
      "File \u001b[0;32m~/fhd/langchain-apis/.venv/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for GoogleGenerativeAIEmbeddings\nmodel\n  field required (type=value_error.missing)\n__root__\n  Did not find google_api_key, please add an environment variable `GOOGLE_API_KEY` which contains it, or pass `google_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = GoogleGenerativeAIEmbeddings().embed_query\n",
    "convo_vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
    "\n",
    "convo_retriever = convo_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "convo_memory = VectorStoreRetrieverMemory(retriever=convo_retriever)\n",
    "# convo_memory.save_context({\"Human\": \"My name is Tom\"}, {\"Ai\": \"Hi Tom\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.chains import ConversationChain\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "def save_response(parsed_response):\n",
    "    print('hey')\n",
    "    convo_memory.save_context({\"Ai\":parsed_response},{\"Human\":\"\"})\n",
    "    return parsed_response\n",
    "\n",
    "def save_question(params):\n",
    "   print(params['question'])\n",
    "   convo_memory.save_context({\"Human\":params['question']},{\"Ai\":\"Processing...\"})\n",
    "   return params['question']\n",
    "\n",
    "def retrieval_complete(params):\n",
    "    print('retrieval complete')\n",
    "    return params\n",
    "\n",
    "history_chain = (\n",
    "    {\n",
    "        \"history\": itemgetter(\"question\") | convo_retriever | format_docs,\n",
    "        \"context\": itemgetter(\"question\") | retriever | format_docs, \n",
    "        \"question\": save_question ,\n",
    "        \"ability\": itemgetter(\"ability\")\n",
    "    }\n",
    "    | with_history_prompt \n",
    "    | retrieval_complete\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | save_response\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you remember the secret word i told you?\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Do you remember the secret word i told you?\"\n",
    "ai_msg = history_chain.invoke(\n",
    "    {\"ability\":\"ai\",\"question\": question}\n",
    ")\n",
    "# convo_memory.load_memory_variables({\"Human\":\"What is my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
